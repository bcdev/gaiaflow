{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gaiaflow Documentation","text":"<p>Gaiaflow is a local-first MLOps infrastructure tool that simplifies the process  of building, testing, and deploying ML workflows. It provides an opinionated CLI for managing Airflow, MLflow, and other  dependencies, abstracting away complex configurations, and giving you a smooth  developer experience.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Local MLOps Infrastructure via CLI with pre-installed prerequisites.</li> <li>Simplified Airflow + Xcom   Integration with an easy interface for DAG creation.</li> <li>Cookiecutter template for project scaffolding with standardized structure.</li> <li>State-of-the-art services provided:<ul> <li>Apache Airflow</li> <li>MLflow</li> <li>MinIO</li> <li>JupyterLab</li> </ul> </li> <li>Containerization support to package workflow steps as python packages in    Docker images.</li> <li>Deployment support:<ul> <li>Local development</li> <li>Minikube production-like testing locally</li> </ul> </li> <li>Future roadmap:<ul> <li>Model deploy locally and in production</li> <li>Streamlined production deployment documentation</li> <li>Centralized DAG repository for CI/CD pipelines</li> <li>Private onboarding steps for users</li> </ul> </li> </ul>"},{"location":"#gaiaflow-core","title":"Gaiaflow-Core","text":"<p>Gaiaflow library provides a simplified interface to create airflow tasks:</p> <p><code>from gaiaflow.core.create_task import create_task</code></p> <p>The <code>create_task</code> function is a high-level abstraction on top of  Apache Airflow operators.</p> <p>This interface makes it easy and intuitive to develop a DAG in Airflow without worrying about the complexities of Airflow concepts and configurations about  running them in different environments allowing you to define tasks for your DAGs in a uniform way.</p> <p>Instead of writing different operator classes manually for development,  containers, or production deployments, <code>create_task</code>  automatically selects the correct operator and applies consistent configuration based on the <code>mode</code> argument.</p> <p>It supports function execution via a dotted and colon string  (e.g. <code>my_package.module:function</code>) with args/kwargs in any of the supported modes without changing the user code.</p> <p>It is configurable with:</p> <ul> <li>Docker images</li> <li>Secrets</li> <li>Environment variables</li> <li>Retry policies</li> <li>Airflow params</li> </ul> <p>Arguments for: <code>create_task</code></p> Name Type Default Description task_id <code>str</code> \u2013 Unique identifier for the task in the DAG. func_path <code>str</code> \u2013 Path to the Python function to execute. Format: <code>\"module:function_name\"</code>. func_args <code>list</code> <code>[]</code> Positional arguments for the function. The user can also provide output from upstream tasks as args input using the <code>FromTask</code> class. Usage shown below. func_kwargs <code>dict</code> <code>{}</code> Keyword arguments to pass into the function. The user can also provide output from upstream tasks as kwargs input using the <code>FromTask</code> class. Usage shown below. image <code>str</code> <code>None</code> Docker image to run the task in <code>dev_docker</code>, <code>prod_local</code> or <code>prod</code> mode. Ignored in <code>dev</code> mode. mode <code>str</code> <code>\"dev\"</code> Execution mode. Must be one of: <code>[\"dev\", \"dev_docker\", \"prod_local\", \"prod\"]</code>. secrets <code>list</code> <code>None</code> List of secrets (e.g., Kubernetes secrets) required by the task in <code>prod_local</code> or <code>prod</code> mode. env_vars <code>dict</code> <code>{}</code> Environment variables to inject into the task. Can be used in <code>dev_docker</code>, <code>prod_local</code> or <code>prod</code> mode. retries <code>int</code> <code>3</code> Number of times to retry the task on failure. dag <code>airflow.DAG</code> <code>None</code> Optional reference to the DAG object. Used to inherit DAG-level params. Should be passed if <code>params</code> are defined in your DAG"},{"location":"#fromtask","title":"<code>FromTask</code>","text":"<p>When building workflows with Gaiaflow, tasks often depend on the output of a previous task.</p> <p>The FromTask helper makes this easy: it allows you to pull values from upstream  tasks and use them as arguments in downstream tasks.</p> <p>What It Does - Provides a simple way to pass values between tasks without writing manual  XCom  logic. - Works seamlessly inside <code>func_args</code> and <code>func_kwargs</code> of <code>create_task</code>. - Lets you specify:   - Which task to pull from (task)   - Which key inside the upstream task\u2019s return dictionary to extract (key)</p> <p>Example usage:</p> <pre><code>from airflow import DAG\nfrom airflow.utils.task_group import TaskGroup\nfrom datetime import datetime\nfrom gaiaflow.core.create_task import create_task\nfrom gaiaflow.core.operators import FromTask\n\ndefault_args = {\n    \"owner\": \"airflow\",\n}\n\nMODE = \"dev\"  # or \"dev_docker\", \"prod_local\", \"prod\"\n\nwith DAG(\n    \"my_training_dag\",\n    default_args=default_args,\n    description=\"Example DAG using create_task\",\n    schedule=\"0 12 * * *\",\n    start_date=datetime(2025, 1, 1),\n    catchup=False,\n    tags=[\"ml\", \"gaiaflow\"],\n    params={\"experiment\": \"mnist\"}, # This will be available to your task via \n                                    # `kwargs[params]`\n) as dag:\n\n    with TaskGroup(group_id=\"training_group\") as trainer:\n\n        preprocess = create_task(\n            task_id=\"preprocess_data\",\n            func_path=\"my_package:preprocess\",\n            func_kwargs={\"path\": \"/data/raw\"},\n            mode=MODE,\n            dag=dag,\n        )\n\n        train = create_task(\n            task_id=\"train_model\",\n            func_path=\"my_package:train\",\n            func_kwargs={\n                \"epochs\": 10, \n                # The key `preprocessed_path` is the kwarg expected by the train method\n                \"preprocessed_path\": FromTask(\n                    task=\"training_group.preprocess_data\", # This tells which task to pull the value from\n                    key=\"preprocessed_path\", # This tells that we want to extract the value of `preprocessed_path`. \n                    # The previous task must make sure that they return a dict with `preprocessed_path` in it\n                    # like return {\"preprocessed_path\": processed_path}.\n                )},\n            mode=MODE,\n            dag=dag,\n        )\n\n        evaluate = create_task(\n            task_id=\"evaluate_model\",\n            func_path=\"my_package:evaluate\",\n            func_kwargs={\"metrics\": [\"accuracy\", \"f1\"]},\n            mode=MODE,\n            dag=dag,\n        )\n\n        preprocess &gt;&gt; train &gt;&gt; evaluate\n</code></pre>"},{"location":"#best-practices-for-passing-values-from-one-task-to-another","title":"Best Practices for passing values from one task to another","text":"<ul> <li>Always make your task functions return a dictionary of literals. This ensures    downstream tasks can pull values by key.</li> <li>Use clear, descriptive keys like <code>\"preprocessed_path\"</code>, <code>\"model_path\"</code>, <code>\"metrics\"</code>.</li> </ul>"},{"location":"#gaiaflow-behaviour-by-mode","title":"Gaiaflow behaviour by <code>mode</code>","text":"<p>The <code>mode</code> argument in <code>create_task</code> controls where and how your function runs.  Internally, each mode maps to a different Airflow operator, with  environment-specific defaults for networking, MLflow, MinIO, and execution backend.</p> <ol> <li><code>dev</code> \u2192 Local Python execution<ul> <li>Airflow Operator: <code>ExternalPythonOperator</code></li> <li>Runs your function directly on the Python environment created for you in     the docker containers using your dependencies.</li> <li>Ignores the <code>image</code>, <code>secrets</code> and <code>env_vars</code> params.</li> <li>Uses your local <code>.env</code> file for environment variables.</li> <li>You will be using this mode most of the time for iteration, development     and debugging.</li> </ul> </li> <li><code>dev_docker</code> \u2192 Local Docker container execution<ul> <li>Airflow Operator: <code>DockerOperator</code></li> <li>Runs your function inside a Docker container</li> <li>Requires <code>image</code> parameter</li> <li>You can pass in <code>env_vars</code> which are then available as environment      variables.</li> <li>This mode is useful when you want to test the docker image locally      before moving to Kubernetes which helps you in catching bugs early on.</li> </ul> </li> <li><code>prod_local</code> \u2192 Simulates production on your machine using Kubernetes via Local Minikube<ul> <li>Operator: <code>KubernetesPodOperator</code></li> <li>Runs tasks as Kubernetes pods in Minikube.</li> <li>Requires <code>image</code></li> <li>You can pass in <code>env_vars</code> and/or <code>secrets</code> if any. </li> <li>For creating secrets in your minikube cluster, you can run <pre><code>gaiaflow prod-local create-secret -p . --name &lt;your-secret-name&gt; --data SOME_KEY=value --data SOME_SECRET=secret\n</code></pre></li> </ul> </li> <li><code>prod</code> \u2192 Kubernetes in-cluster (production)<ul> <li>Operator: <code>KubernetesPodOperator</code></li> <li>Runs tasks as Kubernetes pods in Minikube.</li> <li>Requires <code>image</code></li> <li>You can pass in <code>env_vars</code> and/or <code>secrets</code> if any. </li> <li>For creating secrets in your minikube cluster (coming soon)</li> </ul> </li> </ol> <p>Quick rule of thumb:</p> <ul> <li>Use <code>dev</code> for rapid iteration and development.</li> <li>Use <code>dev_docker</code> to validate your Docker image locally.</li> <li>Use <code>prod_local</code> for testing end-to-end workflows on production-like settngs.</li> <li>Use <code>prod</code> for production pipelines in the real cluster.</li> </ul>"},{"location":"#user-workflow","title":"User workflow:","text":"<p>A typical user workflow could look like this:</p> <pre><code>flowchart TD\n    A1[Start]--&gt; A\n    A{Prerequisites installed except template?}--&gt;|Yes| D{Which OS?}\n    A --&gt;|No| STOP[Stop]\n\n    D --&gt;|Windows WSL2| E1[Install template via conda/mamba/miniforge prompt&lt;br/&gt;then create env in WSL2 CLI + install gaiaflow]\n    D --&gt;|Linux| E2[Install template + create env&lt;br/&gt;then install gaiaflow]\n    E1 --&gt; M\n    E2 --&gt; M\n\n    M[Start services in Dev Mode&lt;br/&gt;`gaiaflow start dev -p .`] --&gt; N[Experiment in JupyterLab if needed]\n\n    N --&gt; P[Refactor to production code&lt;br/&gt;inside Python package and logging experiements to MLFlow and artifacts to S3 MinIO if needed]\n    P --&gt; Q[Write tests in tests/ folder]\n    Q --&gt; R[Create workflows with create_task mode=dev in Airflow]\n    R --&gt; S[Run &amp; monitor DAG in Airflow]\n    S --&gt; T[Track experiments in MLflow &amp; outputs in MinIO]\n    T --&gt; V{Workflow ran successfully in dev mode?}\n    V --&gt; |Yes| V1{Test your package in docker mode?}\n\n    V1 --&gt; |Yes| V2[Change create_task mode to dev_docker and run gaiaflow dev dockerize -p .]\n\n    V2 --&gt; V3{Docker mode ran successfully?}\n    V3 --&gt; |Yes| V4{Move to production?}\n\n    V4 --&gt; |Yes| X1[Start Prod-Local infra&lt;br/&gt;`gaiaflow prod-local start -p .`]\n\n    V --&gt; |No| V5[Fix bugs]\n    V1 --&gt; |No| STOP[Stop]\n    V3 --&gt; |No| V6[Fix bugs]\n    V4 --&gt; |No| STOP[Stop]\n\n    V5 --&gt; R\n    V6 --&gt; V3\n\n    X1 --&gt; X2[Build Docker image&lt;br/&gt;`gaiaflow prod-local dockerize -p .`]\n    X2 --&gt; X3[Configure secrets if needed]\n    X3 --&gt; Y{Prod-Local Success?}\n    Y2 --&gt; Y\n    Y --&gt;|Yes| Y1[Set create_task mode to prod - Local services are not needed anymore]\n    Y1 --&gt; Z[Deploy to Production Cluster ]\n\n    Y --&gt;|No| Y2[Fix the bugs]\n\n    class A,D,U,Y,V,V1,V3,V4 decision;\n    class E1,F1,G1,H1 windows;\n    class E2,G2,H2 linux;\n    class Z prod;\n    class STOP,STOP2 stop;\n\n    classDef decision fill:#FFDD99,stroke:#E67E22,stroke-width:2px;\n    classDef windows fill:#AED6F1,stroke:#2874A6,stroke-width:2px;\n    classDef linux fill:#ABEBC6,stroke:#1E8449,stroke-width:2px;\n    classDef prod fill:#D7BDE2,stroke:#7D3C98,stroke-width:2px;\n    classDef stop fill:#F5B7B1,stroke:#922B21,stroke-width:2px;\n</code></pre>"},{"location":"cli_ref/","title":"CLI Reference","text":"<p>To see options for any command, add <code>--help</code> at the end of your command.</p> <pre><code>gaiaflow --help\n</code></pre>"},{"location":"cli_ref/#gaiaflow-dev","title":"<code>gaiaflow dev</code>","text":"<p>Manage local development services.</p> <pre><code>gaiaflow dev [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Commands</p> <ul> <li><code>start</code> \u2013 Start Gaiaflow dev services</li> <li><code>stop</code> \u2013 Stop Gaiaflow dev services</li> <li><code>restart</code> \u2013 Restart services</li> <li><code>cleanup</code> \u2013 Remove temporary context and state</li> <li><code>dockerize</code> \u2013 Build a Docker image of your ML package</li> </ul> <p>Example</p> <pre><code>gaiaflow dev start -p /path/to/project --service airflow --jupyter-port 8888\n</code></pre>"},{"location":"cli_ref/#gaiaflow-prod-local","title":"<code>gaiaflow prod-local</code>","text":"<p>Manage production-like services (via Minikube).</p> <p>Commands</p> <ul> <li><code>start</code> \u2013 Start production-like services</li> <li><code>stop</code> \u2013 Stop services</li> <li><code>restart</code> \u2013 Restart services</li> <li><code>dockerize</code> \u2013 Build image inside Minikube</li> <li><code>create-config</code> \u2013 Generate Airflow K8s config (debugging only)</li> <li><code>create-secret</code> \u2013 Create Kubernetes (K8s) secrets</li> <li><code>cleanup</code> \u2013 Remove Minikube-specific resources</li> </ul> <p>Example</p> <pre><code>gaiaflow prod-local start -p /path/to/project --force-new\n</code></pre>"},{"location":"dev_guide/","title":"User Guide - <code>dev</code> and <code>dev_docker</code> mode","text":"<p>Now that you have created a project using the template provided, and have the  prerequisites installed please follow the steps below to start your ML journey.</p>"},{"location":"dev_guide/#0-git-fundamentals","title":"0. Git Fundamentals.","text":"<p>First, we need to initialize a Git repository to make the initial commit. <pre><code>  cd {{ cookiecutter.folder_name }}\n  git init -b main\n  git add .\n  git commit -m \"Initial commit\"\n</code></pre></p> <p>Next, create a repository in Github. Once created, copy the remote repository  URL. Open the terminal with this project as the current working directory. Then, replace the REMOTE-URL with your repo's URL on Github <pre><code>  git remote add origin REMOTE-URL\n</code></pre> Verify if the remote URL was set correctly. <pre><code>  git remote -v\n</code></pre> To push the changes, do the following: <pre><code>  git push origin main\n</code></pre> Now you have created a git repository with an initial commit of this project. To proceed, create a new branch and start working in it. <pre><code>  git checkout -b name-of-your-branch\n</code></pre></p>"},{"location":"dev_guide/#1-create-and-activate-mamba-environment","title":"1. Create and activate mamba environment","text":"<p>You can update the <code>environment.yml</code> to include your libraries, or you can  update them later as well. <pre><code>  mamba env create\n  mamba activate &lt;your-env-name&gt;\n</code></pre></p> <p>If you have created an environment using the steps above, and would like to  update the mamba env after adding new libraries in <code>environment.yml</code>, do this: <pre><code>  mamba env update\n</code></pre> To reflect these changes in Airflow as well, please restart the services as  shown in the next step.</p>"},{"location":"dev_guide/#2-start-the-services","title":"2. Start the services","text":"<p>The following command spins up a docker compose containers for Airflow, MLFLow,  MinIO and a Jupyter Lab service (not in a container).</p> <pre><code>gaiaflow start dev -p .\n</code></pre> <p>NOTE: You can always choose which services you want to start using  <code>--service</code> flag in the command. </p> <p>Check <code>gaiaflow start dev --help</code></p> <p>For e.g., if you just want to start a JupyterLab for starters, run:</p> <pre><code>gaiaflow start dev -p . --service jupyter\n</code></pre>"},{"location":"dev_guide/#3-accessing-the-services","title":"3. Accessing the services","text":"<p>Wait for the services to start (usually takes around 5 mins for the first run)</p> <ul> <li>Airflow UI<ul> <li>URL: http://localhost:8080</li> <li>username: <code>admin</code></li> <li>password: <code>admin</code></li> </ul> </li> <li>Mlflow UI<ul> <li>URL: http://localhost:5000</li> </ul> </li> <li>Minio (Local S3)<ul> <li>URL: http://localhost:9000</li> <li>username: <code>minio</code></li> <li>password: <code>minio123</code></li> </ul> </li> <li>JupyterLab<ul> <li>URL: Opens up JupyterLab automatically at port 8895    (unless you change it using the <code>-j</code> flag)</li> </ul> </li> </ul>"},{"location":"dev_guide/#4-development-workflow","title":"4. Development Workflow","text":""},{"location":"dev_guide/#1-experiment-in-jupyterlab","title":"1. Experiment in JupyterLab","text":"<ul> <li>When services start, <code>JupyterLab</code> opens automatically in your browser.  </li> <li>Navigate to the <code>notebooks/</code> folder and create notebooks to experiment with  data, models, and logging (metrics, params, and artifacts to MLflow).  </li> <li>Starter notebooks are available in the <code>examples/</code> folder:  <ul> <li>Learn how to use MLflow for experiment tracking.  </li> <li>Perform inference on MLflow-logged models.  </li> </ul> </li> <li>If your data comes from S3 (hosted by BC), it\u2019s best to download a small  sample and upload it to your local S3 storage (MinIO) for development and testing. NOTE: This is recommended because there are egress-costs (costs that occur when  the data is pulled out from the AWS ecosystem) for everytime you pull the data.</li> <li>For local datasets, upload them to MinIO as well. Later, when your workflow moves to production, ensure the data is available in S3. <code>aws s3 sync /path/to/source /path/to/target --delete</code> Use <code>--delete</code> if you want the target to look exactly as the source</li> </ul>"},{"location":"dev_guide/#2-refactor-to-production-code","title":"2. Refactor to Production Code","text":"<ul> <li>Once your logic (data ingestion, preprocessing, training, or post-processing)  is ready, refactor it into production code inside the Python package directory.  </li> <li>Modify the files starting with <code>change_me_*</code>.  </li> <li>If you included examples when creating this project, explore files starting  with <code>example_*</code> for guidance and delete them later on.  </li> <li>Important: Functions intended to run as tasks in Airflow must:  <ul> <li>Accept and return small data (strings, numbers, booleans).  </li> <li>Return results wrapped in a <code>dict</code>, so downstream tasks can consume them.  </li> </ul> </li> </ul>"},{"location":"dev_guide/#3-write-tests","title":"3. Write Tests","text":"<ul> <li>Add tests in the <code>tests/</code> directory to validate:  <ul> <li>Data preprocessing methods.  </li> <li>Data schemas, transformations, etc.  </li> </ul> </li> <li>Ensure your tests pass (make them green).  </li> </ul>"},{"location":"dev_guide/#4-create-your-workflows","title":"4. Create Your Workflows","text":"<ul> <li>Navigate to the <code>dags/</code> folder and open the <code>change_me_*</code> files.  </li> <li>These files include instructions on how to define DAGs.  </li> <li>If examples are present, check <code>example_*</code> files to learn how DAGs are structured.  </li> <li>Workflows are created using the provided <code>create_task</code> function.  </li> <li>During early development:  <ul> <li>Use <code>mode = \"dev\"</code>.  </li> <li>To pass environment variables, simply add them to your <code>.env</code> file.  </li> </ul> </li> </ul>"},{"location":"dev_guide/#5-run-and-monitor-your-dag","title":"5. Run and Monitor Your DAG","text":"<ul> <li>Open the Airflow UI.  </li> <li>Find your DAG and trigger it with the \u25b6\ufe0f Trigger DAG button.  </li> <li>Monitor execution status and view logs directly in the UI.  </li> </ul>"},{"location":"dev_guide/#6-validate-outputs","title":"6. Validate Outputs","text":"<ul> <li>Check the MinIO UI to ensure data and artifacts were generated correctly.  </li> </ul>"},{"location":"dev_guide/#7-track-experiments","title":"7. Track Experiments","text":"<ul> <li>While your model is training, open the MLflow UI to track experiments, parameters, metrics, and artifacts.  </li> </ul>"},{"location":"dev_guide/#8-deploy-your-model","title":"8. Deploy Your Model","text":"<ul> <li>Once training is complete, deploy your locally to test it using docker or  test it directly (see next section for details).   </li> </ul>"},{"location":"dev_guide/#9-containerize-for-dev-docker-mode","title":"9. Containerize for Dev-Docker Mode","text":"<ul> <li>If your DAG works in <code>dev</code> mode, you can containerize your package and test  it inside a Docker environment:  </li> </ul> <pre><code>gaiaflow dev dockerize -p .\n</code></pre> <ul> <li>This builds an image for your package</li> <li>Use the generated image name in the <code>image</code> parameter of <code>create_task</code>.</li> <li>Pass environment variables via the <code>env_vars</code> parameter.</li> <li>Set <code>mode = \"dev_docker\"</code> and trigger your workflow again.</li> <li>If everything works, you\u2019re ready to test in a production-like setting.</li> </ul> <p>(Note: This step is optional \u2014 you can skip directly to production-like testing if preferred.)</p>"},{"location":"dev_guide/#5-stopping-the-services","title":"5. Stopping the services","text":"<p>You should stop these container services when you're done working with your  project, need to free up system resources, or want to apply some  updates. To gracefully stop the services, run this in the terminal where you started them:</p> <pre><code>gaiaflow dev stop -p .\n</code></pre>"},{"location":"dev_guide/#6-cleanup","title":"6. Cleanup","text":"<p>When you docker a lot on your local system to build images, it caches the  layers that it builds and overtime, this takes up a lot of memory.  To remove the cache, run this:</p> <pre><code>gaiaflow dev cleanup -p .\n</code></pre>"},{"location":"dev_guide/#mlflow-model-deployment-workflow-locally","title":"MLFlow Model Deployment workflow locally","text":"<p>Once you have a model trained, you can deploy it locally either as container or serve it directly from MinIO S3. We recommend to deploy it as a container as this makes sure that it has its  own environment for serving.</p>"},{"location":"dev_guide/#deploying-model-as-a-container-locally","title":"Deploying Model as a Container locally","text":"<p>Since we have been working with docker containers so far, all the environment  variables have been set for them, but now as we need to deploy them, we would need to export a few variables so that MLFLow has access to them and  can pull the required models from MinIO S3.</p> <pre><code>  export MLFLOW_TRACKING_URI=http://127.0.0.1:5000 \n  export MLFLOW_S3_ENDPOINT_URL=http://127.0.0.1:9000 \n  export AWS_ACCESS_KEY_ID=minio\n  export AWS_SECRET_ACCESS_KEY=minio123\n</code></pre> <p>Once we have this variables exported, find out the <code>run_id</code> or the <code>s3_path</code> of  the model you  want to deploy from the MLFlow UI and run the following command:</p> <p><pre><code>  mlflow models build-docker -m runs:/&lt;run-id&gt;/model -n &lt;name-of-your-container&gt; --enable-mlserver --env-manager conda\n</code></pre> or  <pre><code>  mlflow models build-docker -m &lt;s3_path&gt; -n &lt;name-of-your-container&gt; --enable-mlserver --env-manager conda\n</code></pre></p> <p>After this finishes, you can run the docker container by:</p> <pre><code>  docker run -p 5002:8080 &lt;name-of-your-container&gt; \n</code></pre> <p>Now you have an endpoint ready at <code>127.0.0.1:5002</code>.</p> <p>Have a look at <code>notebooks/examples/mlflow_local_deploy_inference.ipynb</code> for an  example on how to get the predictions.</p>"},{"location":"dev_guide/#deploying-local-inference-server","title":"Deploying local inference server","text":"<p>Prerequisites</p> <ul> <li>Pyenv</li> <li>Make sure standard libraries in linux are upto date.   <pre><code>sudo apt-get update\nsudo apt-get install -y build-essential\nsudo apt-get install --reinstall libffi-dev\n</code></pre></li> <li>Run these commands to export the AWS (Local Minio server running)   <pre><code> export AWS_ACCESS_KEY_ID=minio \n export AWS_SECRET_ACCESS_KEY=minio123\n export MLFLOW_S3_ENDPOINT_URL=http://127.0.0.1:9000\n</code></pre></li> <li>Now we are ready for local inference server. Run this after replacing the  required stuff     <pre><code>mlflow models serve -m s3://mlflow/0/&lt;run_id&gt;/artifacts/&lt;model_name&gt; -h 0.0.0.0 -p 3333\n</code></pre></li> <li>We can now run inference against this server on the <code>/invocations</code> endpoint,</li> <li>Have a look at <code>notebooks/examples/mlflow_local_deploy_inference.ipynb</code> for an  example on how to get the predictions.</li> </ul>"},{"location":"dev_guide/#testing","title":"Testing","text":"<p>The template package comes with an initial template suite of tests.  Please update these tests after you update the code in your package.</p> <p>To test your dags along with your package, run this:</p> <p>In Windows: <pre><code>set AIRFLOW_CONFIG=%cd%\\airflow_test.cfg\n</code></pre> <pre><code>pytest --ignore=logs\n</code></pre></p> <p>In Linux: <pre><code>export AIRFLOW_CONFIG=$(pwd)/airflow_test.cfg\n</code></pre></p> <pre><code>pytest\n</code></pre>"},{"location":"dev_guide/#code-formatting-practices-while-developing-your-package","title":"Code formatting practices while developing your package","text":""},{"location":"dev_guide/#ruff-check-linting","title":"Ruff Check (linting)","text":"<pre><code>ruff check .\n</code></pre>"},{"location":"dev_guide/#ruff-check-with-auto-fix-as-much-as-possible","title":"Ruff Check with Auto-fix (as much as possible)","text":"<pre><code>ruff check .  --fix\n</code></pre>"},{"location":"dev_guide/#ruff-format-code-formatting","title":"Ruff Format (Code formatting)","text":"<pre><code>ruff format .\n</code></pre>"},{"location":"dev_guide/#isort-import-sorting","title":"isort (import sorting)","text":"<pre><code>isort. \n</code></pre>"},{"location":"dev_guide/#optional-creating-your-python-package-distribution","title":"(Optional) Creating your python package distribution","text":"<p>There are two options:</p> <ol> <li>You can use the provided CI workflow <code>.github/workflows/publish.yml</code> which     is triggered everytime you create a <code>Release</code>. If you choose this method,     please add <code>PYPI_API_TOKEN</code> to the secrets for this repository.    (or)</li> <li>You can do it manually as shown below:</li> </ol> <p>First update the <code>pyproject.toml</code> as required for your package.</p> <p>Then install the PyPi build if you  dont have already have it. <pre><code>  pip install build\n</code></pre></p> <p>Then from the root of this project, run: <pre><code>  python -m build\n</code></pre></p> <p>Once this command runs successfully, you can install your package using: <pre><code>  pip install dist/your-package\n</code></pre></p> <p>If you would like to upload your package to PyPi, follow the steps below: 1. Install <code>twine</code> <pre><code>  pip install twine\n</code></pre> 2. Register yourself at PyPi if you have not already. Create an API token that  you will use for uploading it to PyPi 3. Run this and enter your username and API token when prompted <pre><code>  twine upload dist/*\n</code></pre> 4. Now your package should have been uploaded to PyPi. 5. You can test it by: <pre><code>  pip install your-package\n</code></pre></p>"},{"location":"dev_guide/#accessingviewing-these-services-in-pycharm","title":"Accessing/Viewing these services in Pycharm","text":"<p>If you are a Pycharm user, you are amazing!</p> <p>If not, please consider using it as it provides a lot of functionalities in  its community version.</p> <p>Now, let's use one of its features called Services. It is a small hexagonal  button with the play icon inside it. You will find it in one of the tool windows.</p> <p>When you open it, you can add services like Docker and Kubernetes. But for this  framework, we only need Docker.</p> <p>To view the docker service here, first we need to install the Docker Plugin in  Pycharm.</p> <p>To do so, <code>PyCharm settings</code> -&gt; <code>Plugins</code> -&gt; Install Docker plugin from  marketplace</p> <p>Then, reopen the services window, and when you add a new service, you will find  Docker.</p> <p>Just use the default settings.</p>"},{"location":"dev_guide/#troubleshooting-tips","title":"Troubleshooting Tips","text":"<ul> <li>If you get Port already in use, change it with -j or free the port.</li> <li>Use <code>-v</code> to clean up Docker volumes if service states become inconsistent.</li> <li>Logs are saved in the logs/ directory.</li> <li>Please make sure that none of the <code>__init__.py</code> files are completely empty as this creates some issues with mlflow logging. You can literally just add a <code>#</code> to the <code>__init__.py</code> file. This is needed because while serializing the files, empty files have 0 bytes of content and that creates issues with the urllib3 upload to S3 (this happens inside MLFlow)</li> <li>If there are any errors in using the Minikube manager, try restarting it by <code>python minikube_manager.py --restart</code> followed by  <code>python mlops_manager.py --restart</code> to make sure that the changes are synced.</li> <li>If you face an issue with pyenv as such: <code>python-build: defintion not found: 3.12.9</code> then update your python-build definitions by: <code>cd ~/.pyenv/plugins/python-build &amp;&amp; git pull</code></li> </ul>"},{"location":"prod_guide/","title":"User Guide - <code>prod_local</code> and <code>prod</code> mode","text":""},{"location":"prod_guide/#testing-your-dags-using-docker-images-in-prod_local-mode","title":"Testing your DAGs using Docker Images in <code>prod_local</code> mode","text":"<p>Before running your DAGs in production, it\u2019s recommended to test them in  <code>prod_local</code> mode. This simulates how tasks will run on the production cluster: - Each task within the DAG executes in its own isolated container. - You get a realistic, end-to-end test of your workflow. </p>"},{"location":"prod_guide/#to-test-this-you-need-a-few-things","title":"To test this, you need a few things:","text":""},{"location":"prod_guide/#step-0-set-mode","title":"Step 0. Set Mode","text":"<p>In your DAG code, set the <code>create_task</code> parameter to use <code>prod_local</code> mode:  </p> <pre><code>create_task(..., mode = \"prod_local\")\n</code></pre>"},{"location":"prod_guide/#step-1-start-local-kubernetes-minikube","title":"Step 1. Start Local Kubernetes (Minikube)","text":"<p>Spin up a lightweight local Kubernetes cluster: <pre><code>gaiaflow prod-local start -p .\n</code></pre></p> <p>This may take a few minutes while infrastructure is provisioned.</p>"},{"location":"prod_guide/#step-2-build-your-docker-image","title":"Step 2. Build your Docker image","text":"<p>Make your project\u2019s Docker image available inside Minikube:</p> <pre><code>gaiaflow prod-local dockerize -p .\n</code></pre>"},{"location":"prod_guide/#step-3-create-secrets-optional","title":"Step 3. Create Secrets (Optional)","text":"<p>If you need to pass sensitive environment variables that shouldn\u2019t go directly into <code>env_vars</code>, store them as Kubernetes secrets:</p> <pre><code>gaiaflow prod-local create-secret -p . \\\n--name &lt;your-secret-name&gt; \\\n--data SOME_KEY=value \\\n--data SOME_SECRET=secret\n</code></pre>"},{"location":"prod_guide/#step-4-configure-airflow-temporary-step","title":"Step 4. Configure Airflow (Temporary Step)","text":"<p>Tell Airflow where your Minikube cluster is  (this step may be automated in the future):</p> <pre><code>gaiaflow prod-local create-config -p .\n</code></pre> <p>Once these steps are done, you are ready to test your DAG in production-like  setting.</p> <p>Head over to the Airflow UI, you should now see your DAG using <code>KubernetesPodOperator</code>.  - Trigger the DAG from the UI - Watch the logs and wait for the DAG to finish execution</p> <p>If everything works smoothly here, your workflow is ready to move to production.</p>"},{"location":"prod_guide/#bonus-testing","title":"BONUS testing","text":"<p>This is optional but in case you want to test each task separately using the  docker image that you created, you can do so.</p> <p>To test this, you would need to pass in the arguments that that specific task  requires.</p> <p>To test your each task, do this:</p> <pre><code>docker run --rm -e \\\nFUNC_PATH=\"your_module.your_function\" \\\n-e FUNC_KWARGS='{\"param1\": \"value1\", \"param2\": \"value2\"}' \\\nyour-image-name:tag\n</code></pre> <p>Doing this test is to ensure that your task functions are idempotent (i.e. they will always return the same result, no matter how many time you run it!) and  that they can run in an isolated environment in production.</p> <p>Test this for all your tasks individually.</p>"},{"location":"prod_guide/#running-airflow-dags-in-production","title":"Running Airflow DAGs in Production","text":"<p>To run your DAGs in a production Airflow deployment, a few things need  to be in place.</p>"},{"location":"prod_guide/#notes","title":"Notes","text":"<ul> <li>These setup steps are required once per project (except task secrets, which may change).  </li> <li>MLflow support in production is coming soon</li> </ul>"},{"location":"prod_guide/#1-centralized-dag-repository-cdr","title":"1. Centralized DAG Repository (CDR)","text":"<p>Your team or company should maintain a Centralized DAG Repository (CDR). For BC, it is available here This repository collects DAGs from multiple projects and makes them available to Airflow.  </p>"},{"location":"prod_guide/#2-required-secrets","title":"2. Required Secrets","text":"<p>You\u2019ll need to configure the following secrets in your repository:  </p> <ul> <li><code>PYPI_API_TOKEN</code> \u2192 Create from the PyPI website and add as a repo secret  </li> <li><code>CODECOV_TOKEN</code> \u2192 Create from the Codecov website and add as a repo secret  </li> <li><code>CDR_PAT</code> \u2192 A GitHub Personal Access Token (PAT) used to interact with CDR  </li> </ul>"},{"location":"prod_guide/#creating-the-cdr_pat","title":"Creating the <code>CDR_PAT</code>","text":"<ol> <li>Go to your GitHub Account Settings </li> <li>Navigate to Developer Settings (last option in the sidebar)  </li> <li>Create a Personal access token (classic) </li> <li>Select only the <code>repo</code> permissions  </li> <li>Generate the token  </li> <li>Copy and save it somewhere safe (e.g., KeePassXC)  </li> <li>Add it to your project as a GitHub Secret:  </li> <li>Go to Repository Settings \u2192 Secrets and Variables \u2192 Actions \u2192 New repository secret </li> <li>Name: <code>CDR_PAT</code> </li> <li>Secret: paste your token  </li> <li>Click Add Secret </li> </ol>"},{"location":"prod_guide/#3-task-secrets","title":"3. Task Secrets","text":"<p>Any secrets required by your tasks must also be made available to the cluster running Airflow. (coming soon)</p>"},{"location":"prod_guide/#4-github-release","title":"4. GitHub Release","text":"<p>To enable Airflow running in production to access your DAGs and your task packaged as an image, you must make a release following semantic versioning.</p> <p>To release your package, you can do it via Github UI.</p> <p>To make the release work successfully, you have to request the AWS access credentials from the infra team.</p> <p>Add this as a GitHub repository secret so your CI can use it.  </p>"},{"location":"prod_guide/#youre-ready-for-production","title":"You\u2019re Ready for Production","text":"<p>With all prerequisites in place: 1. Create a GitHub release. 2. The automation pipeline runs the steps above. 3. Within a few minutes, your DAG will appear in the Airflow UI, ready to be triggered.  </p>"},{"location":"start/","title":"Getting Started with Gaiaflow","text":"<p>This guide will help you set up your environment, install dependencies, and  generate your first project using the GaiaFlow template.</p> <p>The following section provides an overview of the services provided by the  Gaiaflow framework. For first-time users, we recommend that you read/skim  through this to get an idea of what Gaiaflow can currently offer.</p> <p>If you would like to start right away, then get started here</p>"},{"location":"start/#overview","title":"Overview","text":"<p>Gaiaflow integrates essential MLOps tools:</p> <ul> <li>Cookiecutter: For providing a standardized project structure</li> <li>Apache Airflow: For orchestrating ML pipelines and workflows</li> <li>MLflow: For experiment tracking and model registry</li> <li>JupyterLab: For interactive development and experimentation</li> <li>MinIO: For local object storage for ML artifacts</li> <li>Minikube: For local lightweight Kubernetes cluster</li> </ul>"},{"location":"start/#mlops-components","title":"MLOps Components","text":"<p>Before you get started, let's explore the tools that we are using for this  standardized MLOps framework </p>"},{"location":"start/#0-cookiecutter","title":"0. Cookiecutter","text":"<p>Purpose: Project scaffolding and template generation</p> <ul> <li>Provides a standardized way to create ML projects with predefined structures.</li> <li>Ensures consistency across different ML projects</li> <li>Get started here</li> </ul>"},{"location":"start/#1-apache-airflow","title":"1. Apache Airflow","text":"<p>Purpose: Workflow orchestration</p> <ul> <li>Manages and schedules data pipelines.</li> <li>Automates end-to-end ML workflows, including data ingestion, training, deployment and re-training.</li> <li>Provides a user-friendly web interface for tracking task execution's status.</li> </ul>"},{"location":"start/#key-concepts-in-airflow","title":"Key Concepts in Airflow","text":""},{"location":"start/#dag-directed-acyclic-graph","title":"DAG (Directed Acyclic Graph)","text":"<p>A DAG is a collection of tasks organized in a structure that reflects their  execution order. DAGs do not allow for loops, ensuring deterministic scheduling  and execution.</p>"},{"location":"start/#task","title":"Task","text":"<p>A Task represents a single unit of work within a DAG. Each task is an instance of an Operator. Gaiaflow provides create_task for the ease of defining tasks in a DAG.</p>"},{"location":"start/#operator","title":"Operator","text":"<p>Operators define the type of work to be done. They are templates that  encapsulate logic.</p> <p>Common Operators:</p> <ul> <li> <p>PythonOperator: Executes a Python function.</p> </li> <li> <p>BashOperator: Executes bash commands.</p> </li> <li> <p>KubernetesPodOperator: Executes code inside a Kubernetes pod.</p> </li> <li> <p>DummyOperator: No operation \u2014 used for DAG design.</p> </li> </ul> <p>For ease of use of Airflow, we have created a function <code>create_task</code> that  allows the user to create these tasks without worrying about which operator to use. Read more here</p>"},{"location":"start/#scheduler","title":"Scheduler","text":"<p>The scheduler is responsible for triggering DAG runs based on a schedule.  It evaluates DAGs, resolves dependencies, and queues tasks for execution.</p>"},{"location":"start/#xcom-cross-communication","title":"XCom (Cross-Communication)","text":"<p>A lightweight mechanism for passing small data between tasks. Data is stored  in Airflow\u2019s metadata DB and fetched using Jinja templates or Python.</p>"},{"location":"start/#airflow-ui","title":"Airflow UI","text":"<ul> <li>DAGs (Directed Acyclic Graphs): A workflow representation in Airflow. You  can enable, disable, and trigger DAGs from the UI.</li> <li>Graph View: Visual representation of task dependencies.</li> <li>Tree View: Displays DAG execution history over time.</li> <li>Task Instance: A single execution of a task in a DAG.</li> <li>Logs: Each task's execution details and errors.</li> <li>Code View: Shows the Python code of a DAG.</li> <li>Trigger DAG: Manually start a DAG run.</li> <li>Pause DAG: Stops automatic DAG execution.</li> </ul> <p>Common Actions</p> <ul> <li>Enable a DAG: Toggle the On/Off button.</li> <li>Manually trigger a DAG: Click Trigger DAG \u25b6\ufe0f.</li> <li>View logs: Click on a task instance and select Logs.</li> <li>Restart a failed task: Click Clear to rerun a specific task.</li> </ul>"},{"location":"start/#2-mlflow","title":"2. MLflow","text":"<p>Purpose: Experiment tracking and model management</p> <ul> <li>Tracks and records machine learning experiments, including hyperparameters, performance metrics, and model artifacts.</li> <li>Facilitates model versioning and reproducibility.</li> <li>Supports multiple deployment targets, including cloud platforms, Kubernetes, and on-premises environments.</li> </ul>"},{"location":"start/#core-components","title":"Core Components","text":""},{"location":"start/#tracking","title":"Tracking","text":"<p>Allows logging of metrics, parameters, artifacts, and models for every  experiment.</p>"},{"location":"start/#models","title":"Models","text":"<p>MLflow models are saved in a standard format that supports deployment to  various serving platform.</p>"},{"location":"start/#model-registry","title":"Model Registry","text":"<p>Central hub for managing ML models where one can register and version models.</p>"},{"location":"start/#mlflow-ui","title":"MLFlow UI","text":"<ul> <li>Experiments: Group of runs tracking different versions of ML models.</li> <li>Runs: A single execution of an ML experiment with logged parameters,  metrics, and artifacts.</li> <li>Parameters: Hyperparameters or inputs logged during training.</li> <li>Metrics: Performance indicators like accuracy or loss.</li> <li>Artifacts: Files such as models, logs, or plots.</li> <li>Model Registry: Centralized storage for trained models with versioning.</li> </ul> <p>Common Actions</p> <ul> <li>View experiment runs: Go to Experiments &gt; Select an experiment</li> <li>Compare runs: Select multiple runs and click Compare.</li> <li>View parameters and metrics: Click on a run to see details.</li> <li>View registered model: Under Artifacts, select a model and click Register  Model.</li> </ul> <p>For a quick MLFLow tutorial, see <code>notebooks/examples/mlflow_introduction.ipynb</code></p>"},{"location":"start/#3-jupyterlab","title":"3. JupyterLab","text":"<p>Purpose: Interactive development environment</p> <ul> <li>Provides an intuitive and interactive web-based interface for exploratory data analysis, visualization, and model development.</li> </ul>"},{"location":"start/#4-minio","title":"4. MinIO","text":"<p>Purpose: Object storage for ML artifacts</p> <ul> <li>Acts as a cloud-native storage solution for datasets and models.</li> <li>Provides an S3-compatible API for seamless integration with ML tools.</li> <li>Suitable for Local development iterations using a portion of the data</li> </ul>"},{"location":"start/#5-minikube","title":"5. Minikube","text":"<p>Purpose: Local Kubernetes cluster for development &amp; testing</p> <ul> <li>Allows you to run a single-node Kubernetes cluster locally.</li> <li>Simulates a production-like environment to test Airflow DAGs end-to-end.</li> <li>Great for validating KubernetesExecutor, and Dockerized task behavior before deploying to a real cluster.</li> <li>Mimics production deployment without the cost or risk of real cloud infrastructure.</li> </ul>"},{"location":"start/#getting-started","title":"Getting Started","text":"<p>Before starting, ensure you have the following installed from the  links provided:</p> <p>If you face any issues, please check out the troubleshooting section</p>"},{"location":"start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose</li> <li>Mamba - Please make sure you  install <code>Python 3.12</code> as this repository has been tested with that version.</li> <li>Minikube on Linux</li> <li>Minikube on Windows</li> <li>Create a Gaiaflow cookiecutter project</li> <li>Gaiaflow</li> </ul>"},{"location":"start/#docker-and-docker-compose-plugin-installation","title":"Docker and Docker compose plugin Installation","text":"<p>For Linux users: please follow the steps mentioned in this link</p> <p>For Windows users: please follow the steps mentioned in this link</p> <p>This should install both Docker and Docker compose plugin. You can verify the installation by these commands <pre><code>   docker --version\n   docker compose version\n</code></pre> and output would be something like: <pre><code>  Docker version 27.5.1, build 9f9e405\n  Docker Compose version v2.32.4\n</code></pre> This means now you have successfully installed Docker.</p>"},{"location":"start/#gaiaflow-installation","title":"Gaiaflow Installation","text":"<p>In Windows, install <code>gaiaflow</code> using WSL2 terminal in a new mamba/conda environment.</p> <p>You can install Gaiaflow directly via pip:</p> <p><code>pip install gaiaflow</code></p> <p>Verify installation:</p> <p><code>gaiaflow --help</code></p>"},{"location":"start/#gaiaflow-cookiecutter","title":"Gaiaflow Cookiecutter","text":"<p>Once you created the project template using the Gaiaflow cookiecutter, it will contain the following files and folders.</p> <p>Any files or folders marked with <code>*</code> are off-limits\u2014no need to change, modify,  or even worry about them. Just focus on the ones without the mark!</p> <p>Any files or folders marked with <code>^</code> can be extended, but carefully. <pre><code>\u251c\u2500\u2500 .github/             # GitHub Actions workflows (you are provided with a starter CI)\n\u251c\u2500\u2500 dags/                # Airflow DAG definitions \n\u2502                          (you can either define dags using a config-file (dag-factory)\n\u2502                           or use Python scripts.)\n\u251c\u2500\u2500 notebooks/           # JupyterLab notebooks\n\u251c\u2500\u2500 your_package/                  \n\u2502   \u2502                     (For new projects, it would be good to follow this standardized folder structure.\n\u2502   \u2502                      You are of course allowed to add anything you like to it.)\n\u2502   \u251c\u2500\u2500 dataloader/      # Your Data loading scripts\n\u2502   \u251c\u2500\u2500 train/           # Your Model training scripts\n\u2502   \u251c\u2500\u2500 preprocess/      # Your Feature engineering/preprocessing scripts\n\u2502   \u251c\u2500\u2500 postprocess/     # Your Postprocessing model output scripts\n\u2502   \u251c\u2500\u2500 model/           # Your Model defintion\n\u2502   \u251c\u2500\u2500 model_pipeline/  # Your Model Pipeline to be used for inference\n\u2502   \u2514\u2500\u2500 utils/           # Utility functions\n\u251c\u2500\u2500 tests/               # Unit and integration tests\n\u251c\u2500\u2500 data/                # If you have data locally, move it here and use it so that airflow has access to it.\n\u251c\u2500\u2500 README.md            # Its a readme. Feel to change it!\n\u251c\u2500\u2500 CHANGES.md           # You put your changelog for every version here.\n\u251c\u2500\u2500 pyproject.toml       # Config file containing your package's build information and its metadata\n\u251c\u2500\u2500 .env * ^             # Your environment variables that docker compose and python scripts can use (already added to .gitignore)\n\u251c\u2500\u2500 .gitignore * ^       # Files to ignore when pushing to git.\n\u251c\u2500\u2500 environment.yml      # Libraries required for local mlops and your project\n\u2514\u2500\u2500 airflow_test.cfg *   # This file is needed for testing your airflow dags.\n</code></pre></p> <p>In your package, you are provided with scripts starting with <code>change_me_*</code>. Please have a look at the comments in these files before starting.</p> <p>If you chose to have examples for dags and the package, you will find the files  starting with <code>example_*</code>. Please have a look at these files to get more info  and to get started.</p>"},{"location":"start/#next-step","title":"Next step","text":"<p>Once the pre-requisites are done, please follow along here.</p>"},{"location":"start/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>If you are windows, please use the <code>miniforge prompt</code> commandline.</p> </li> <li> <p>If you face issue like <code>Docker Daemon not started</code>, start it using: <pre><code>  sudo systemctl start docker\n</code></pre> and try the docker commands again in a new terminal.</p> </li> <li> <p>If you face an issue as follows: <code>Got permission denied while trying to connect to the Docker daemon socket at  unix:///var/run/docker.sock:</code>, do the following <pre><code>  sudo chmod 666 /var/run/docker.sock\n</code></pre> and try the docker commands again in a new terminal.</p> </li> <li> <p>If you face an issue like <code>Cannot connect to the Docker daemon at unix:///home//.docker/desktop/docker.sock.  Is the docker daemon running?</code>, it is likely because of you have two contexts of docker running.</p> </li> </ol> <p>To view the docker contexts, <pre><code>   docker context ls\n</code></pre> This will show the list of docker contexts. Check if default is enabled (it  should have a * beside it) If not, you might probably have desktop as your context enabled. To confirm which context you are in: <pre><code>   docker context show\n</code></pre></p> <p>To use the default context, do this: <pre><code>   docker context use default\n</code></pre></p> <p>Check for the following file: <pre><code>  cat ~/.docker/config.json\n</code></pre> If it is empty, all good, if not, it might be something like this: <pre><code>  {\n    \"auths\": {},\n    \"credsStore\": \"desktop\"\n  }\n</code></pre> Completely move this file away from this location or delete it and try running  docker again.</p> <ol> <li>If you face some permissions issues on some files like <code>Permission Denied</code>,  as a workaround, please use this and let us know so that we can update this  repo. <pre><code>  sudo chmod 666 &lt;your-filename&gt; \n</code></pre></li> </ol> <p>If you face any other problems not mentioned above, please reach out to us.</p>"}]}